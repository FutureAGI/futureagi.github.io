<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size:32px;
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
<h2> 
	<title>OmniRL: Large-Scale Meta-Training in Randomized Worlds </title>
	<meta property="og:title" content="OmniRL:基于随机世界中大规模元训练的上下文强化学习" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<span style="font-size:24px"><a href='https://futureagi.github.io/'>[回到主目录]</a></span>
	<br>
	<center>
		<span style="font-size:36px">OmniRL:基于随机世界中大规模元训练的上下文强化学习</span><br>
		<span style="font-size:24px">OmniRL: Large-Scale Meta-Training in Randomized Worlds</span>
		<table align=center width=850px>
			<tr>
				<td>
	            <center>
				<strong style="color: red; font-weight: bold;">授AI以鱼，不如授AI以渔</strong>
				</td>
			</tr>
		</table>
		<table align=center width=600px>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2502.02869'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='hhttps://github.com/airs-cuhk/airsoul/tree/main/projects/OmniRL'>[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
<hr>
<center>
	<table align=center width=850px>
	<tr>
	<td width=260px>
	<center>
	<img class="round" style="width:100%" src="./imgs/OmniRL_Framework.gif" alt="OmniRL简介"/>
	<p class="caption">OmniRL:实时上下文强化学习基座模型</p>
	</center>
	</td>
	</tr>
	</table>
</center>

<h2>预训练和元训练：授模型以鱼和授模型以渔</h2>
<br>
训练最早以教会模型完成多种多样的任务为出发点，在大规模预训练中，模型除了记住这些训练任务本身，涌现出的最重要的能力就是上下文学习（In-Context Learning），这种能力能够帮助大模型通过提示信息来构建完成新任务的能力。
<br>
OmniRL提出大规模元训练，和预训练不同之处在于，OmniRL目标不是记忆训练任务的技能本身，而是学习如何进行强化学习的过程。元学习，又叫学习学习的方法(Learning To Learn)，早在80年代被提出。但OmniRL论文认为，缺乏大规模任务和长序列支撑的元学习，容易实际陷入“任务识别”的模式：模型只是记住了训练的环境，在推理时通过识别处于哪一个环境“激活”对应的技能。这种模式不具备对未见任务和分布外任务的泛化性。

<h2>随机世界：AnyMDP</h2>
<br>
<center>
	<table align=center width=850px>
	<tr>
	<td width=260px>
	<center>
	<img class="round" style="height:100px" src="./imgs/AnyMDP_Visualization.png" alt="AnyMDP生成的随机世界示例"/>
	<p class="caption"> AnyMDP生成的随机世界示例。点的颜色标识状态的平均奖励，线的深度标识状态之间的平均转移概率</p>
	</center>
	</td>
	</tr>
	</table>
</center>
AnyMDP基于马尔科夫决策过程（Markov Decision Process, MDP）构建随机迁移概率和奖励函数，能够迅速低成本生成海量的，可规模化的环境用于元强化学习。我们生成了共计超过50万个不同任务，并基于这些任务合成了超过100亿时间步的数据用于元训练。单个序列的时间步长最长超过100万。



<h2>首次利用上下文学习统一多强化学习和模仿学习</h2>
<br>
OmniRL提出同时利用先验信息和后验奖励（Feedback）进行上下文学习，使得该模型可以自己在不同学习模式间根据需求切换。图2展示了在随机世界中训练的OmniRL模型，仅通过上下文学习，不依赖于任何梯度优化，在冷启动，或者预先给定一段演示轨迹（包括专家演示，以及较差的演示），能通过在线强化学习（Online-RL），离线强化学习 (Offline-RL)，模仿学习 (IL)的自主切换，达到较好表现，证明上下文学习有巨大的灵活性。进一步，还能在演示的基础上，进一步通过自主探索持续提升自身能力。

<center>
	<table align=center width=850px>
	<tr>
	<td width=260px>
	<center>
	<img class="round" style="height:60px" src="./imgs/OmniRLDemoCliff.gif" alt="Cliff"/>
	<img class="round" style="height:60px" src="./imgs/OmniRLDemoLake.gif" alt="Lake"/>
	<img class="round" style="height:60px" src="./imgs/OmniRLDemoPend.gif" alt="Pendulum"/>
	<img class="round" style="height:60px" src="./imgs/OmniRLSwitch.gif" alt="Switch"/>
	<br>
	<img class="round" style="width:90%" src="./imgs/result_gym.png" alt="gym"/>
	<p class="caption">OmniRL在完全未见过的Gymnasium环境中的表现</p>
	</center>
	</td>
	</tr>
	</table>
</center>
<br>
OmniRL训练的智能体甚至可以完成多智能体协作任务。通过把对方的状态引入到观测，它可以完成类似Switch这种简单任务。这类任务要求智能体展现不同的行为模式来实现协作。而通过模型的上下文学习和适应能力，两个OmniRL控制的智能体能够有效完成上述任务。

<h2>首次揭示出数据多样性和序列长度重要性根源</h2>
<br>
<center>
	<table align=center width=850px>
	<tr>
	<td width=260px>
	<center>
	<img class="round" style="width:40%" src="./imgs/convergence_seen_ns16.png" alt="Seen_Training"/>
	<img class="round" style="width:40%" src="./imgs/convergence_unseen_ns16.png" alt="Unseen_Training"/>
	<p class="caption">模型的位置损失和元训练的步数，以及上下文长度的关系</p>
	</center>
	</td>
	</tr>
	</table>
</center>
OmniRL使用数千万参数的Transformer和高效线性注意力结构来进行建模。训练任务数超过50万个，时间步数超过一百万。OmniRL在实验中对比相同数据量，但来自不同任务数量的效果，发现任务数量不够多时，模型会转向记忆 + 环境识别模式，即把所有训练的环境储存在参数记忆中，通过上下文进行快速的辨识。这种模式下，智能体能否以更少的样本适应训练过程中见过的环境，但却不能泛化到未见环境。而任务数量充分时，才能激发通用上下文学习能力，这种能力可以有效泛化到未见任务，但对于所有任务都需要更长的上下文学习周期。这个结论一定程度说明:
<ul>  
<li><strong style="color: red; font-weight: bold;">数据的完整性和多样性比数据的绝对准确性（absolute fidelity）更重要</strong>.即使采用失真的数据，通过提升上下文学习的泛化性，也有可能更好泛化到真实任务。</li>  
<li><strong style="color: red; font-weight: bold;">长序列建模和长时记忆是通用学习能力的自然选择</strong>.在训练任务数量增加时，模型在学习过程中自然选择不去记忆任务关联的知识，而是只记忆学习方法，从而导致对训练集中出现的任务也需要花费更长适应时间。这正是大规模元学习的特点。</li>  
</ul>


<h2>线性自注意力机制体现出效率和长序列表现上的明显优势</h2>
<br>
<center>
	<table align=center width=850px>
	<tr>
	<td width=260px>
	<center>
	<img class="round" style="width:30%" src="./imgs/validation_gsa_t16_t128.png" alt="gsa"/>
	<img class="round" style="width:30%" src="./imgs/validation_t16_gsa_trnxl.png" alt="task_16"/>
	<img class="round" style="width:30%" src="./imgs/validation_t64_gsa_trnxl.png" alt="task_64"/>
	<p class="caption">模型的位置损失和元训练的步数，以及上下文长度的关系</p>
	</center>
	</td>
	</tr>
	</table>
</center>
OmniRL还首次展示出线性注意力机制的优势。随着问题规模增加，上下文长度需要同步增加，Transformer的效率瓶颈愈发明显。相比之下，线性注意力机制在效率和长序列建模上都有明显优势，相对于滑窗注意力机制，也在长时序段显示出了非常明显的效果优势。证明AnyMDP提供了非常好的长序列的评测环境。


<h2>面向下一代通用具身智能体的技术探索</h2>
<br>
我们的最终目标是实现对任意环境都能完全自主探索和学习的智能体。对于具身智能意义更加重大。大语言模型通过参数记忆捕捉了大量常识，百科和数理逻辑，构成了其零样本能力的基础。但具身智能面对多样的环境，任务以及复杂的本体异构性，常识很难成为解决具身问题的基础。我们认为，自主学习能力和长时记忆将是通用具身智能体的关键。
<br>
<h3>和当前大语言模型的长时序推理和思维链的异同点。</h3>当前OmniRL更多侧重系统1（直觉思维）的学习能力，后者侧重系统2（逻辑思维和规划）本身。不论系统1还是系统2能力的学习和提升，当前主流大模型都没有探索足够充分，OmniRL则在这部分填补了很多空白。

</body>
</html>